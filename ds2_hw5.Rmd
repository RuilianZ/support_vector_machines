---
title: "Data Science II Homewrk 5"
author: "Roxy Zhang"
date: "5/4/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(caret)
library(mlbench)
library(ISLR)
library(e1071) # tune svm
library(kernlab) # implement svm
library(factoextra) # visualization
library(gridExtra) # arrange plots in one page
library(corrplot) 
library(RColorBrewer) # generate colors for heatmap
library(gplots) # flexible heatmap
```

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.align = "center", cache = TRUE, 
                      fig.width = 6, fig.asp = 0.6, out.width = "90%")
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Data import and partition

```{r}
auto = read_csv("auto.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  distinct() %>% 
  mutate(
    cylinders = as.factor(cylinders),
    origin = case_when(origin == "1" ~ "American",
                       origin == "2" ~ "European",
                       origin == "3" ~ "Japanese"),
    origin = as.factor(origin),
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low", "high")
  ) %>% 
  as.data.frame()

skimr::skim_without_charts(auto)
```

```{r}
# split the dataset into two parts: training data (70%) and test data (30%)

set.seed(0504)

indexTrain = createDataPartition(y = auto$mpg_cat,
                                 p = 0.7,
                                 list = FALSE)
```



## Support Vector Machines

### Linear Kernel

Fit a support vector classifier (linear kernel) to the training data. The linear kernel provides a linear decision boundary.

```{r}
set.seed(0504)

# using package e1071
linear_svm = tune.svm(mpg_cat ~ .,
                      data = auto[indexTrain, ],
                      kernel = "linear",
                      cost = exp(seq(-5, 2, len = 50)),
                      scale = TRUE)

# plot misclassification error based on cross validation against tuning parameter(cost)
plot(linear_svm)

# optimal tuning parameter with minimum cross-validation error
linear_svm$best.parameters

# best model
best_linear_svm = linear_svm$best.model
summary(best_linear_svm)
```

From the results above, the optimum tuning parameter is achieved when cost is 0.368, which minimizes the cross-validation error.


```{r}
# calculate training error rate from confusion matrix
confusionMatrix(data = linear_svm$best.model$fitted, 
                reference = auto$mpg_cat[indexTrain])
```

Accuracy is 90.94%, therefore training error rate is 9.06%. We can also do the calculation: the optimal linear support vector classifier with linear kernel incorrectly classifies 25 out of 276 training observations, giving a 9.06% error rate.

```{r}
set.seed(0504)

linear_svm_pred = predict(best_linear_svm, newdata = auto[-indexTrain, ])

# calculate test error rate from confusion matrix
confusionMatrix(data = linear_svm_pred, 
                reference = auto$mpg_cat[-indexTrain])
```

Accuracy is 91.38%, therefore test error rate is 8.62%. Calculation by hand: the classifier incorrectly classifies 10 out of 116 observations, giving a 8.62% error rate.


### Radial Kernel

Fit a support vector machine with a radial kernel to the training data. This gives a nonlinear decision boundary.

```{r}
set.seed(0504)

# using package e1071
radial_svm = tune.svm(mpg_cat ~ .,
                      data = auto[indexTrain, ],
                      kernel = "radial",
                      cost = exp(seq(-3, 8, len = 50)),
                      gamma = exp(seq(-4, 4, len = 20)),
                      scale = TRUE)

plot(radial_svm, transform.y = log, transform.x = log, color.palette = terrain.colors)

radial_svm$best.parameters

best_radial_svm = radial_svm$best.model
summary(radial_svm$best.model)
```

From the results above, the optimum tuning parameter is achieved when gamma is 0.349 cost is 2.262, which minimizes the cross-validation error.  

```{r}
# calculate training error rate from confusion matrix
confusionMatrix(data = radial_svm$best.model$fitted, 
                reference = auto$mpg_cat[indexTrain])
```

Accuracy is 94.93%, therefore training error rate is 5.07%. We can also do the calculation: the optimal linear support vector classifier with radial kernel incorrectly classifies 14 out of 276 training observations, giving a 5.07% error rate.

```{r}
set.seed(0504)

radial_svm_pred = predict(best_radial_svm, newdata = auto[-indexTrain, ])

# calculate test error rate from confusion matrix
confusionMatrix(data = radial_svm_pred, 
                reference = auto$mpg_cat[-indexTrain])
```

Accuracy is 97.41%, therefore test error rate is 2.59%. Calculation by hand: the classifier incorrectly classifies 3 out of 116 observations, giving a 2.59% error rate.

For support vector machine with radial kernel, the training and test error rate are both lower than that of svm with linear kernel. So the model with radial kernel performs better.




## Hierachical Clustering

### Without scaling

```{r}
data(USArrests)

arrests = USArrests %>% as.data.frame()

dim(arrests)

skimr::skim_without_charts(arrests)
```

```{r}
# hierarchical clustering with complete linkage and Euclidean distance (without scaling)
hc = hclust(dist(arrests), method = "complete")
```

```{r}
# cut the dendrogram at a height that results in three distinct clusters
fviz_dend(hc, k = 3,    
          cex = 0.3, 
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)
```

The cluster on the left includes some populous states, such as New York, California, Florida, North Carolina, Illinois, etc.  
The middle cluster includes some Southern states, such as Missouri, Arkansas, Tennessee, but also some others, like Washington, Massachusetts and New Jersey. 
The cluster on the right contains some less populous states, like Ohio, Utah, Kentucky, and others.  
There seems not be any clear pattern pattern of the clustering based on geography, but there might be some grouping tendencies based on population of the states. 


### With scaling

```{r}
# scale and center data
arrests_scaled = scale(arrests, center = TRUE, scale = TRUE)

# hierarchical clustering with complete linkage and Euclidean distance
hc_scaled = hclust(dist(arrests_scaled), method = "complete")
```

```{r}
# cut the dendrogram at a height that results in three distinct clusters
fviz_dend(hc_scaled, k = 3,    
          cex = 0.3, 
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)
```

We can see that there are more states in the left cluster than other 2 clusters. And the distribution of the states is quite different from the dendrogram without clustering. 


### Discussion

Scaling the variables change the clustering results.  
 
With scaling, one cluster contains South Dakota, West Virginia, any many other of the less populous states; another cluster contains California, Nevada, Texas, New York, and primarily more populous states in major urban metro areas; and a third cluster with Alaska, Alabama, Louisiana, Georgia, and a number of other mostly Southern U.S. states. This is quite different from the dendrogram plotted without scaling.

This is because the clustering algorithms requires the method of calculating distance to be specified, as we are using Euclidean distance here. Without scaling numeric variables, the variables with greater magnitudes might be put more importance. In this case, if we do the classification without scaling, we're more likely to cluster based on `assault` than other variables, since it has greater values.  

In my opinion, the variables should be scaled before the inter-observation dissimilarities are computed in order to ensure the variables are of comparable units.



