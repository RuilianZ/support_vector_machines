---
title: "Data Science II Homewrk 5"
author: "Roxy Zhang"
date: "5/4/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(caret)
library(mlbench)
library(ISLR)
library(e1071) # tune svm
library(kernlab) # implement svm
library(factoextra) # visualization
library(gridExtra) # arrange plots in one page
library(corrplot) 
library(RColorBrewer) # generate colors for heatmap
library(gplots) # flexible heatmap
```

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      fig.align = "center", cache = TRUE, 
                      fig.width = 6, fig.asp = 0.6, out.width = "90%")
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Data import and partition

```{r}
auto = read_csv("auto.csv") %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  distinct() %>% 
  mutate(
    cylinders = as.factor(cylinders),
    origin = case_when(origin == "1" ~ "American",
                       origin == "2" ~ "European",
                       origin == "3" ~ "Japanese"),
    origin = as.factor(origin),
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, "low", "high")
  ) %>% 
  as.data.frame()
```

```{r}
# split the dataset into two parts: training data (70%) and test data (30%)

set.seed(0504)

indexTrain = createDataPartition(y = auto$mpg_cat,
                                 p = 0.7,
                                 list = FALSE)
```



## Support Vector Machines

### Linear Kernel

Fit a support vector classifier (linear kernel) to the training data. The linear kernel provides a linear decision boundary.

```{r}
set.seed(0504)

# using package e1071
linear_svc = tune.svm(mpg_cat ~ .,
                      data = auto[indexTrain, ],
                      kernel = "linear",
                      cost = exp(seq(-5, 2, len = 50)),
                      scale = TRUE)

# plot misclassification error based on cross validation against tuning parameter(cost)
plot(linear_svc)

# optimal tuning parameter with minimum cross-validation error
linear_svc$best.parameters

# best model
best_linear_svc = linear_svc$best.model

summary(best_linear_svc)
```

From the results above, the optimum tuning parameter is achieved when cost is 1.535, which minimizes the cross-validation error.


```{r}
# calculate training error rate from confusion matrix
confusionMatrix(data = linear_svc$best.model$fitted, 
                reference = auto$mpg_cat[indexTrain])
```

Accuracy is 91.67%, therefore training error rate is 8.33%.

```{r}
set.seed(0504)

linear_svc_pred = predict(best_linear_svc, newdata = auto[-indexTrain, ])

# calculate test error rate from confusion matrix
confusionMatrix(data = linear_svc_pred, 
                reference = auto$mpg_cat[-indexTrain])
```

Accuracy is 93.1%, therefore test error rate is 6.9%.


### Radio Kernel

Fit a support vector machine with a radial kernel to the training data. This gives a nonlinear decision boundry.

```{r}

```

